{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3170 Artificial Intelligence | Praktikum\n",
    "\n",
    "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 21\n",
    "\n",
    "Group Members:\n",
    "- Denise Felicia Tiowanni (13522013)\n",
    "- Erdianti WIga Putri Andini (13522053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import other libraries if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [],
   "source": [
    "# import data dari google drive\n",
    "test_data = 'https://drive.google.com/uc?id=1k7kmkB-3vkrU5CTDhEVBOxV2VXrnL5ue'\n",
    "train_data = 'https://drive.google.com/uc?id=1BE2Wt0hd0QZTcHWDJN1f94in-Kq0C6AO'\n",
    "\n",
    "test = pd.read_csv(test_data)\n",
    "train = pd.read_csv(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdSor5sdIYGs"
   },
   "source": [
    "# 1. Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGiGPVYNIoWk"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvx-gT3bLfHM"
   },
   "source": [
    "# 2. Split Training Set and Validation Set\n",
    "\n",
    "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
    "\n",
    "Note: For training, you should use the data contained in the `train.csv` given by the TA. The `test.csv` data is only used for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data dari google drive\n",
    "test_data = 'https://drive.google.com/uc?id=1k7kmkB-3vkrU5CTDhEVBOxV2VXrnL5ue'\n",
    "train_data = 'https://drive.google.com/uc?id=1BE2Wt0hd0QZTcHWDJN1f94in-Kq0C6AO'\n",
    "\n",
    "test = pd.read_csv(test_data)\n",
    "train = pd.read_csv(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4yWCUFFBLfHM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: (12000, 19), Validation Set Size: (3000, 19)\n"
     ]
    }
   ],
   "source": [
    "target_col = ['Status']\n",
    "cat_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "indices_to_drop = [i for i, val in enumerate(cat_columns) if val in target_col]\n",
    "cat_columns = np.delete(cat_columns, indices_to_drop)\n",
    "num_columns = train.select_dtypes(include=['int64', 'float64']).columns.tolist()   \n",
    "\n",
    "# split data\n",
    "x_train = train.drop(target_col, axis=1)\n",
    "y_train = train[target_col]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "print(f\"Training Set Size: {x_train.shape}, Validation Set Size: {x_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC14lmo_LfHN"
   },
   "source": [
    "# 3. Data Cleaning and Preprocessing\n",
    "\n",
    "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
    "\n",
    "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
    "\n",
    "For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5rksSMAWICY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             15000 non-null  int64  \n",
      " 1   N_Days         15000 non-null  float64\n",
      " 2   Drug           8450 non-null   object \n",
      " 3   Age            15000 non-null  float64\n",
      " 4   Sex            15000 non-null  object \n",
      " 5   Ascites        8453 non-null   object \n",
      " 6   Hepatomegaly   8448 non-null   object \n",
      " 7   Spiders        8441 non-null   object \n",
      " 8   Edema          15000 non-null  object \n",
      " 9   Bilirubin      15000 non-null  float64\n",
      " 10  Cholesterol    6626 non-null   float64\n",
      " 11  Albumin        15000 non-null  float64\n",
      " 12  Copper         8340 non-null   float64\n",
      " 13  Alk_Phos       8444 non-null   float64\n",
      " 14  SGOT           8441 non-null   float64\n",
      " 15  Tryglicerides  6575 non-null   float64\n",
      " 16  Platelets      14416 non-null  float64\n",
      " 17  Prothrombin    14984 non-null  float64\n",
      " 18  Stage          15000 non-null  float64\n",
      " 19  Status         15000 non-null  object \n",
      "dtypes: float64(12), int64(1), object(7)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "\n",
      "test data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             10000 non-null  int64  \n",
      " 1   N_Days         9999 non-null   float64\n",
      " 2   Drug           5632 non-null   object \n",
      " 3   Age            10000 non-null  float64\n",
      " 4   Sex            10000 non-null  object \n",
      " 5   Ascites        5635 non-null   object \n",
      " 6   Hepatomegaly   5628 non-null   object \n",
      " 7   Spiders        5627 non-null   object \n",
      " 8   Edema          10000 non-null  object \n",
      " 9   Bilirubin      10000 non-null  float64\n",
      " 10  Cholesterol    4375 non-null   float64\n",
      " 11  Albumin        10000 non-null  float64\n",
      " 12  Copper         5560 non-null   float64\n",
      " 13  Alk_Phos       5626 non-null   float64\n",
      " 14  SGOT           5626 non-null   float64\n",
      " 15  Tryglicerides  4343 non-null   float64\n",
      " 16  Platelets      9610 non-null   float64\n",
      " 17  Prothrombin    9990 non-null   float64\n",
      " 18  Stage          10000 non-null  float64\n",
      "dtypes: float64(12), int64(1), object(6)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"train data:\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\ntest data:\")\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values in train data:\n",
      "Drug             6550\n",
      "Ascites          6547\n",
      "Hepatomegaly     6552\n",
      "Spiders          6559\n",
      "Cholesterol      8374\n",
      "Copper           6660\n",
      "Alk_Phos         6556\n",
      "SGOT             6559\n",
      "Tryglicerides    8425\n",
      "Platelets         584\n",
      "Prothrombin        16\n",
      "dtype: int64\n",
      "\n",
      "missing values in test data:\n",
      "N_Days              1\n",
      "Drug             4368\n",
      "Ascites          4365\n",
      "Hepatomegaly     4372\n",
      "Spiders          4373\n",
      "Cholesterol      5625\n",
      "Copper           4440\n",
      "Alk_Phos         4374\n",
      "SGOT             4374\n",
      "Tryglicerides    5657\n",
      "Platelets         390\n",
      "Prothrombin        10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "missing_train = train.isnull().sum()\n",
    "missing_test = test.isnull().sum()\n",
    "\n",
    "print(\"missing values in train data:\")\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "# percentage\n",
    "for column in missing_train[missing_train > 0].index:\n",
    "    print(f\"{column}: {missing_train[column] / train.shape[0] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nmissing values in test data:\")\n",
    "print(missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:\n",
      "['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']\n",
      "\n",
      "numerical features:\n",
      "['id', 'N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n"
     ]
    }
   ],
   "source": [
    "# separate categorical and numerical features\n",
    "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"categorical features:\")\n",
    "print(categorical_features)\n",
    "\n",
    "print(\"\\nnumerical features:\")\n",
    "print(numerical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in Drug:\n",
      "Drug\n",
      "D-penicillamine    4515\n",
      "Placebo            3935\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Sex:\n",
      "Sex\n",
      "F    14405\n",
      "M      595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Ascites:\n",
      "Ascites\n",
      "N    8027\n",
      "Y     426\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Hepatomegaly:\n",
      "Hepatomegaly\n",
      "N        4652\n",
      "Y        3795\n",
      "158.0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Spiders:\n",
      "Spiders\n",
      "N    6598\n",
      "Y    1843\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Edema:\n",
      "Edema\n",
      "N    13874\n",
      "S      737\n",
      "Y      389\n",
      "Name: count, dtype: int64\n",
      "\n",
      "unique values in Status:\n",
      "Status\n",
      "C     10117\n",
      "D      4525\n",
      "CL      358\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check unique values in categorical features\n",
    "for col in categorical_features:\n",
    "    print(f\"unique values in {col}:\")\n",
    "    print(train[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of each column in numerical:\n",
      "id : 0.0\n",
      "N_Days : 5.723226588086048\n",
      "Age : 0.2501326957918793\n",
      "Bilirubin : 4.228548344977254\n",
      "Cholesterol : 4.499543223972586\n",
      "Albumin : -0.30018987991930035\n",
      "Copper : 3.115699180745672\n",
      "Alk_Phos : 3.1538951588374986\n",
      "SGOT : 65.28128415449646\n",
      "Tryglicerides : 2.368980642200841\n",
      "Platelets : 2.340967199225039\n",
      "Prothrombin : 1.2436597583866906\n",
      "Stage : -0.4751521529128604\n"
     ]
    }
   ],
   "source": [
    "# check for skewness\n",
    "print(\"Skewness of each column in numerical:\")\n",
    "for column in numerical_features:\n",
    "    print(column, \":\", train[column].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ctVzt5DLfHd"
   },
   "source": [
    "# 3. Compile Preprocessing Pipeline\n",
    "\n",
    "All of the preprocessing classes or functions defined earlier will be compiled in this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZlncSVjJG6"
   },
   "source": [
    "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHraoW_7LfHd"
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # Note: You can add or delete preprocessing components from this pipeline\n",
    "\n",
    "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
    "#                  (\"featurecreator\", FeatureCreator()),\n",
    "#                  (\"scaler\", FeatureScaler()),\n",
    "#                  (\"encoder\", FeatureEncoder())])\n",
    "\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s56aFFxLfHd"
   },
   "outputs": [],
   "source": [
    "# # Your code should work up until this point\n",
    "# train_set = pipe.fit_transform(train_set)\n",
    "# val_set = pipe.transform(val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXoCqMztjhr-"
   },
   "source": [
    "or create your own here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OoZ3oXEj2CW"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3adbZXLfHe"
   },
   "source": [
    "# 4. Modeling and Validation\n",
    "\n",
    "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the probability for each class in the `Status` feature (`Status_C`, `Status_CL`, `Status_D`). Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnhMNbBILfHf"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV6ICmFmlqjk"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW0bMzkDLfHf"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_XwsN_-LfHg"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLDtIkPdLfHg"
   },
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ6_x1LKLfHh"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP4TQDeRUUE0"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIjExgsPUYmM"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg8aMBvoUWM-"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E70wbTkUV58"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoH2u6fOLfHh"
   },
   "source": [
    "## Notes for improvements\n",
    "\n",
    "- **Visualize the model evaluation result**\n",
    "\n",
    "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
    "\n",
    "- **Explore the hyperparameters of your models**\n",
    "\n",
    "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
    "\n",
    "- **Cross-validation**\n",
    "\n",
    "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)\n",
    "\n",
    "- **Ensemble methods**\n",
    "\n",
    "Ensemble methods are powerful machine learning techniques that combine the predictions of multiple models (often referred to as base learners or weak learners) to create a stronger, more accurate predictive model. The idea behind ensemble methods is that by aggregating the opinions of multiple models, you can reduce the impact of individual model errors and improve overall prediction performance. (Hint: bagging, boosting, stacking, voting)\n",
    "\n",
    "- **Model interpretation**\n",
    "\n",
    "Model interpretation is the process of understanding and explaining the inner workings of a machine learning model, particularly its decision-making process. Interpretation helps data scientists, stakeholders, and end-users gain insights into why a model makes certain predictions or classifications. Model interpretation is crucial for building trust in machine learning systems, identifying biases, and extracting actionable information from models. (Hint: Feature importance, PDP, SHAP Values, etc)\n",
    "\n",
    "- **Explore other models**\n",
    "\n",
    "There are a lot of ML models that you can use in this usecase. Try to explore and use them to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li4l53DjLfHh"
   },
   "source": [
    "## Submission\n",
    "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
    "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
    "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
    "3. Retrain the model on the preprocessed training set\n",
    "4. Predict the test set\n",
    "5. Make sure the submission contains the `id`, `Status_C`, `Status_CL`, `Status_D` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeqnfWc-LfHi"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-jXvKOpLfHi"
   },
   "source": [
    "# 6. Error Analysis\n",
    "\n",
    "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
    "\n",
    "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
    "- To each models I have tried, which performs the best and what could be the reason?\n",
    "- Is it better for me to impute or drop the missing data? Why?\n",
    "- Does feature scaling help improve my model performance?\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWL3nEAELfHj"
   },
   "source": [
    "`Provide your analysis here`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
