{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IF3170 Artificial Intelligence | Praktikum\n",
        "\n",
        "This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"
      ],
      "metadata": {
        "id": "uR1JW69eLfG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group Number: xx\n",
        "\n",
        "Group Members:\n",
        "- Name (NIM)\n",
        "- Name (NIM)"
      ],
      "metadata": {
        "id": "ucbaI5rBLtjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "GwzsfETHLfHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import other libraries if needed"
      ],
      "metadata": {
        "id": "jZJU5W_4LfHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset"
      ],
      "metadata": {
        "id": "OKbjLIdYLfHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "-IWFJ-gdLfHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Exploratory Data Analysis\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis."
      ],
      "metadata": {
        "id": "YdSor5sdIYGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "bGiGPVYNIoWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Split Training Set and Validation Set\n",
        "\n",
        "Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split.\n",
        "\n",
        "Note: For training, you should use the data contained in the `train.csv` given by the TA. The `test.csv` data is only used for kaggle submission."
      ],
      "metadata": {
        "id": "gvx-gT3bLfHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training set and validation set here, store into variables train_set and val_set.\n",
        "# Remember to also keep the original training set before splitting. This will come important later.\n",
        "# train_set, val_set = ..."
      ],
      "metadata": {
        "id": "4yWCUFFBLfHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning and Preprocessing\n",
        "\n",
        "This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n",
        "\n",
        "By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n",
        "\n",
        "For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"
      ],
      "metadata": {
        "id": "IC14lmo_LfHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "5rksSMAWICY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Compile Preprocessing Pipeline\n",
        "\n",
        "All of the preprocessing classes or functions defined earlier will be compiled in this step."
      ],
      "metadata": {
        "id": "-ctVzt5DLfHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."
      ],
      "metadata": {
        "id": "S_ZlncSVjJG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# # Note: You can add or delete preprocessing components from this pipeline\n",
        "\n",
        "# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n",
        "#                  (\"featurecreator\", FeatureCreator()),\n",
        "#                  (\"scaler\", FeatureScaler()),\n",
        "#                  (\"encoder\", FeatureEncoder())])\n",
        "\n",
        "# train_set = pipe.fit_transform(train_set)\n",
        "# val_set = pipe.transform(val_set)"
      ],
      "metadata": {
        "id": "jHraoW_7LfHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Your code should work up until this point\n",
        "# train_set = pipe.fit_transform(train_set)\n",
        "# val_set = pipe.transform(val_set)"
      ],
      "metadata": {
        "id": "9s56aFFxLfHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or create your own here"
      ],
      "metadata": {
        "id": "SXoCqMztjhr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "7OoZ3oXEj2CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling and Validation\n",
        "\n",
        "Modelling is the process of building your own machine learning models to solve specific problems, or in this assignment context, predicting the probability for each class in the `Status` feature (`Status_C`, `Status_CL`, `Status_D`). Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."
      ],
      "metadata": {
        "id": "9A3adbZXLfHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "ZnhMNbBILfHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "KV6ICmFmlqjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "nW0bMzkDLfHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "C_XwsN_-LfHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ID3"
      ],
      "metadata": {
        "id": "TLDtIkPdLfHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "gZ6_x1LKLfHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "iP4TQDeRUUE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "bIjExgsPUYmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "pg8aMBvoUWM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "0E70wbTkUV58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes for improvements\n",
        "\n",
        "- **Visualize the model evaluation result**\n",
        "\n",
        "This will help you to understand the details more clearly about your model's performance. From the visualization, you can see clearly if your model is leaning towards a class than the others. (Hint: confusion matrix, ROC-AUC curve, etc.)\n",
        "\n",
        "- **Explore the hyperparameters of your models**\n",
        "\n",
        "Each models have their own hyperparameters. And each of the hyperparameter have different effects on the model behaviour. You can optimize the model performance by finding the good set of hyperparameters through a process called **hyperparameter tuning**. (Hint: Grid search, random search, bayesian optimization)\n",
        "\n",
        "- **Cross-validation**\n",
        "\n",
        "Cross-validation is a critical technique in machine learning and data science for evaluating and validating the performance of predictive models. It provides a more **robust** and **reliable** evaluation method compared to a hold-out (single train-test set) validation. Though, it requires more time and computing power because of how cross-validation works. (Hint: k-fold cross-validation, stratified k-fold cross-validation, etc.)\n",
        "\n",
        "- **Ensemble methods**\n",
        "\n",
        "Ensemble methods are powerful machine learning techniques that combine the predictions of multiple models (often referred to as base learners or weak learners) to create a stronger, more accurate predictive model. The idea behind ensemble methods is that by aggregating the opinions of multiple models, you can reduce the impact of individual model errors and improve overall prediction performance. (Hint: bagging, boosting, stacking, voting)\n",
        "\n",
        "- **Model interpretation**\n",
        "\n",
        "Model interpretation is the process of understanding and explaining the inner workings of a machine learning model, particularly its decision-making process. Interpretation helps data scientists, stakeholders, and end-users gain insights into why a model makes certain predictions or classifications. Model interpretation is crucial for building trust in machine learning systems, identifying biases, and extracting actionable information from models. (Hint: Feature importance, PDP, SHAP Values, etc)\n",
        "\n",
        "- **Explore other models**\n",
        "\n",
        "There are a lot of ML models that you can use in this usecase. Try to explore and use them to solve this problem."
      ],
      "metadata": {
        "id": "LoH2u6fOLfHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n",
        "1. Create a new pipeline instance identical to the first in Data Preprocessing\n",
        "2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n",
        "3. Retrain the model on the preprocessed training set\n",
        "4. Predict the test set\n",
        "5. Make sure the submission contains the `id`, `Status_C`, `Status_CL`, `Status_D` column."
      ],
      "metadata": {
        "id": "Li4l53DjLfHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here"
      ],
      "metadata": {
        "id": "LeqnfWc-LfHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Error Analysis\n",
        "\n",
        "Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n",
        "\n",
        "- Does my model perform better in predicting one class than the other? If so, why is that?\n",
        "- To each models I have tried, which performs the best and what could be the reason?\n",
        "- Is it better for me to impute or drop the missing data? Why?\n",
        "- Does feature scaling help improve my model performance?\n",
        "- etc..."
      ],
      "metadata": {
        "id": "R-jXvKOpLfHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Provide your analysis here`"
      ],
      "metadata": {
        "id": "tWL3nEAELfHj"
      }
    }
  ]
}